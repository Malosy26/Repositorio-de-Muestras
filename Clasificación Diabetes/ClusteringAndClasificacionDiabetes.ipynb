{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(r'D:\\Spark\\spark-3.5.1-bin-hadoop3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-OPSBONI:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark_ejercicio</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x27c9b4db790>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"pyspark_ejercicio\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 17.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = spark.read.csv(path= 'Data/Diabetes.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Diabetes_012: double (nullable = true)\n",
      " |-- HighBP: integer (nullable = true)\n",
      " |-- HighChol: double (nullable = true)\n",
      " |-- CholCheck: integer (nullable = true)\n",
      " |-- BMI: double (nullable = true)\n",
      " |-- Smoker: double (nullable = true)\n",
      " |-- Stroke: double (nullable = true)\n",
      " |-- HeartDiseaseorAttack: double (nullable = true)\n",
      " |-- PhysActivity: integer (nullable = true)\n",
      " |-- Fruits: integer (nullable = true)\n",
      " |-- Veggies: integer (nullable = true)\n",
      " |-- HvyAlcoholConsump: integer (nullable = true)\n",
      " |-- AnyHealthcare: integer (nullable = true)\n",
      " |-- NoDocbcCost: double (nullable = true)\n",
      " |-- GenHlth: double (nullable = true)\n",
      " |-- MentHlth: double (nullable = true)\n",
      " |-- PhysHlth: double (nullable = true)\n",
      " |-- DiffWalk: double (nullable = true)\n",
      " |-- Sex: integer (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Education: double (nullable = true)\n",
      " |-- Income: double (nullable = true)\n",
      "\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 3.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Diabetes_012            0\n",
       "HighBP                  0\n",
       "HighChol                0\n",
       "CholCheck               0\n",
       "BMI                     0\n",
       "Smoker                  0\n",
       "Stroke                  0\n",
       "HeartDiseaseorAttack    0\n",
       "PhysActivity            0\n",
       "Fruits                  0\n",
       "Veggies                 0\n",
       "HvyAlcoholConsump       0\n",
       "AnyHealthcare           0\n",
       "NoDocbcCost             0\n",
       "GenHlth                 0\n",
       "MentHlth                0\n",
       "PhysHlth                0\n",
       "DiffWalk                0\n",
       "Sex                     0\n",
       "Age                     0\n",
       "Education               0\n",
       "Income                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.toPandas().isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 = no diabetes, 1 = prediabetes, 2 = diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|Diabetes_012| count|\n",
      "+------------+------+\n",
      "|         0.0|197191|\n",
      "|         1.0|  5619|\n",
      "|         2.0| 33568|\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupBy(\"Diabetes_012\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+--------+---------+----+------+------+--------------------+------------+------+-------+-----------------+-------------+-----------+-------+--------+--------+--------+---+---+---------+------+\n",
      "|Diabetes_012|HighBP|HighChol|CholCheck| BMI|Smoker|Stroke|HeartDiseaseorAttack|PhysActivity|Fruits|Veggies|HvyAlcoholConsump|AnyHealthcare|NoDocbcCost|GenHlth|MentHlth|PhysHlth|DiffWalk|Sex|Age|Education|Income|\n",
      "+------------+------+--------+---------+----+------+------+--------------------+------------+------+-------+-----------------+-------------+-----------+-------+--------+--------+--------+---+---+---------+------+\n",
      "|         0.0|     0|     1.0|        1|15.0|   1.0|   0.0|                 0.0|           0|     1|      1|                0|            1|        0.0|    5.0|    10.0|    20.0|     0.0|  0| 11|      4.0|   5.0|\n",
      "|         0.0|     0|     0.0|        1|24.0|   1.0|   0.0|                 0.0|           0|     0|      0|                0|            1|        0.0|    3.0|     0.0|     0.0|     1.0|  1| 13|      5.0|   6.0|\n",
      "|         0.0|     1|     0.0|        1|40.0|   1.0|   0.0|                 0.0|           1|     1|      1|                0|            1|        0.0|    3.0|     5.0|    25.0|     1.0|  0| 10|      4.0|   8.0|\n",
      "|         0.0|     1|     1.0|        1|27.0|   1.0|   0.0|                 0.0|           0|     0|      1|                0|            1|        0.0|    4.0|    25.0|     0.0|     0.0|  0| 10|      5.0|   3.0|\n",
      "|         0.0|     1|     1.0|        1|30.0|   0.0|   0.0|                 0.0|           0|     1|      1|                0|            1|        0.0|    2.0|     0.0|     0.0|     0.0|  0|  7|      4.0|   6.0|\n",
      "|         0.0|     1|     1.0|        1|36.0|   1.0|   0.0|                 0.0|           0|     0|      0|                0|            1|        0.0|    4.0|     0.0|     0.0|     0.0|  1| 10|      4.0|   8.0|\n",
      "|         0.0|     1|     1.0|        1|27.0|   1.0|   0.0|                 0.0|           0|     1|      1|                0|            1|        0.0|    3.0|     0.0|     2.0|     1.0|  0| 13|      6.0|   6.0|\n",
      "|         0.0|     1|     1.0|        1|35.0|   1.0|   0.0|                 0.0|           1|     1|      0|                0|            1|        0.0|    3.0|     5.0|     3.0|     0.0|  0| 11|      4.0|   5.0|\n",
      "|         0.0|     1|     1.0|        1|30.0|   0.0|   0.0|                 0.0|           1|     1|      0|                0|            1|        0.0|    2.0|     0.0|    10.0|     0.0|  0| 10|      6.0|   8.0|\n",
      "|         0.0|     0|     0.0|        1|31.0|   0.0|   0.0|                 0.0|           1|     1|      1|                0|            1|        0.0|    1.0|     0.0|     2.0|     0.0|  1| 11|      6.0|   8.0|\n",
      "|         0.0|     0|     0.0|        1|31.0|   1.0|   0.0|                 0.0|           1|     1|      1|                0|            1|        0.0|    4.0|     5.0|     0.0|     0.0|  0| 10|      5.0|   6.0|\n",
      "|         0.0|     1|     0.0|        1|29.0|   0.0|   0.0|                 0.0|           1|     1|      1|                0|            1|        0.0|    4.0|     0.0|     0.0|     0.0|  1| 11|      4.0|   5.0|\n",
      "|         0.0|     1|     0.0|        1|27.0|   1.0|   1.0|                 0.0|           1|     0|      0|                0|            1|        0.0|    3.0|     7.0|    15.0|     0.0|  0|  7|      5.0|   5.0|\n",
      "|         0.0|     1|     0.0|        1|29.0|   1.0|   0.0|                 0.0|           1|     1|      1|                0|            1|        0.0|    3.0|     0.0|    30.0|     0.0|  1| 12|      6.0|   8.0|\n",
      "|         0.0|     0|     0.0|        1|22.0|   0.0|   0.0|                 0.0|           1|     1|      1|                0|            1|        0.0|    1.0|     0.0|     0.0|     0.0|  0|  9|      6.0|   8.0|\n",
      "|         0.0|     1|     1.0|        1|26.0|   1.0|   0.0|                 0.0|           0|     1|      1|                0|            1|        0.0|    3.0|     0.0|     0.0|     1.0|  0| 13|      5.0|   8.0|\n",
      "|         0.0|     1|     1.0|        1|26.0|   1.0|   0.0|                 0.0|           0|     1|      1|                0|            1|        0.0|    3.0|     0.0|     0.0|     1.0|  0| 12|      5.0|   2.0|\n",
      "|         0.0|     0|     0.0|        1|28.0|   0.0|   0.0|                 0.0|           1|     1|      1|                0|            1|        0.0|    2.0|     0.0|     0.0|     0.0|  1|  8|      6.0|  10.0|\n",
      "|         0.0|     1|     1.0|        1|30.0|   1.0|   0.0|                 0.0|           0|     1|      1|                0|            1|        1.0|    4.0|    30.0|    15.0|     1.0|  0|  8|      5.0|   7.0|\n",
      "|         0.0|     1|     1.0|        1|30.0|   1.0|   0.0|                 0.0|           1|     0|      1|                0|            1|        0.0|    4.0|    25.0|     2.0|     1.0|  0|  9|      4.0|   7.0|\n",
      "+------------+------+--------+---------+----+------+------+--------------------+------------+------+-------+-----------------+-------------+-----------+-------+--------+--------+--------+---+---+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.filter(col('Diabetes_012') == 0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos hacer clustering y a analizar los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"handleInvalid: How to handle invalid data (NULL and NaN values). Options are 'skip' (filter out rows with invalid data), 'error' (throw an error), or 'keep' (return relevant number of NaN in the output). Column lengths are taken from the size of ML Attribute Group, which can be set using `VectorSizeHint` in a pipeline before `VectorAssembler`. Column lengths can also be inferred from first rows of the data since it is safe to do so but only in case of 'error' or 'skip'). (default: error)\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_assembler = VectorAssembler()\n",
    "vector_assembler.explainParam(\"handleInvalid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handleInvalid: How to handle invalid data (NULL and NaN values). Options are 'skip' (filter out rows with invalid data), 'error' (throw an error), or 'keep' (return relevant number of NaN in the output). Column lengths are taken from the size of ML Attribute Group, which can be set using `VectorSizeHint` in a pipeline before `VectorAssembler`. Column lengths can also be inferred from first rows of the data since it is safe to do so but only in case of 'error' or 'skip'). (default: error)\n",
      "inputCols: input column names. (undefined)\n",
      "outputCol: output column name. (default: VectorAssembler_5a1cfb01b047__output)\n"
     ]
    }
   ],
   "source": [
    "print(vector_assembler.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class VectorAssembler in module pyspark.ml.feature:\n",
      "\n",
      "class VectorAssembler(pyspark.ml.wrapper.JavaTransformer, pyspark.ml.param.shared.HasInputCols, pyspark.ml.param.shared.HasOutputCol, pyspark.ml.param.shared.HasHandleInvalid, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.JavaMLWritable)\n",
      " |  VectorAssembler(*, inputCols: Optional[List[str]] = None, outputCol: Optional[str] = None, handleInvalid: str = 'error')\n",
      " |  \n",
      " |  A feature transformer that merges multiple columns into a vector column.\n",
      " |  \n",
      " |  .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> df = spark.createDataFrame([(1, 0, 3)], [\"a\", \"b\", \"c\"])\n",
      " |  >>> vecAssembler = VectorAssembler(outputCol=\"features\")\n",
      " |  >>> vecAssembler.setInputCols([\"a\", \"b\", \"c\"])\n",
      " |  VectorAssembler...\n",
      " |  >>> vecAssembler.transform(df).head().features\n",
      " |  DenseVector([1.0, 0.0, 3.0])\n",
      " |  >>> vecAssembler.setParams(outputCol=\"freqs\").transform(df).head().freqs\n",
      " |  DenseVector([1.0, 0.0, 3.0])\n",
      " |  >>> params = {vecAssembler.inputCols: [\"b\", \"a\"], vecAssembler.outputCol: \"vector\"}\n",
      " |  >>> vecAssembler.transform(df, params).head().vector\n",
      " |  DenseVector([0.0, 1.0])\n",
      " |  >>> vectorAssemblerPath = temp_path + \"/vector-assembler\"\n",
      " |  >>> vecAssembler.save(vectorAssemblerPath)\n",
      " |  >>> loadedAssembler = VectorAssembler.load(vectorAssemblerPath)\n",
      " |  >>> loadedAssembler.transform(df).head().freqs == vecAssembler.transform(df).head().freqs\n",
      " |  True\n",
      " |  >>> dfWithNullsAndNaNs = spark.createDataFrame(\n",
      " |  ...    [(1.0, 2.0, None), (3.0, float(\"nan\"), 4.0), (5.0, 6.0, 7.0)], [\"a\", \"b\", \"c\"])\n",
      " |  >>> vecAssembler2 = VectorAssembler(inputCols=[\"a\", \"b\", \"c\"], outputCol=\"features\",\n",
      " |  ...    handleInvalid=\"keep\")\n",
      " |  >>> vecAssembler2.transform(dfWithNullsAndNaNs).show()\n",
      " |  +---+---+----+-------------+\n",
      " |  |  a|  b|   c|     features|\n",
      " |  +---+---+----+-------------+\n",
      " |  |1.0|2.0|NULL|[1.0,2.0,NaN]|\n",
      " |  |3.0|NaN| 4.0|[3.0,NaN,4.0]|\n",
      " |  |5.0|6.0| 7.0|[5.0,6.0,7.0]|\n",
      " |  +---+---+----+-------------+\n",
      " |  ...\n",
      " |  >>> vecAssembler2.setParams(handleInvalid=\"skip\").transform(dfWithNullsAndNaNs).show()\n",
      " |  +---+---+---+-------------+\n",
      " |  |  a|  b|  c|     features|\n",
      " |  +---+---+---+-------------+\n",
      " |  |5.0|6.0|7.0|[5.0,6.0,7.0]|\n",
      " |  +---+---+---+-------------+\n",
      " |  ...\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      VectorAssembler\n",
      " |      pyspark.ml.wrapper.JavaTransformer\n",
      " |      pyspark.ml.wrapper.JavaParams\n",
      " |      pyspark.ml.wrapper.JavaWrapper\n",
      " |      pyspark.ml.base.Transformer\n",
      " |      pyspark.ml.param.shared.HasInputCols\n",
      " |      pyspark.ml.param.shared.HasOutputCol\n",
      " |      pyspark.ml.param.shared.HasHandleInvalid\n",
      " |      pyspark.ml.param.Params\n",
      " |      pyspark.ml.util.Identifiable\n",
      " |      pyspark.ml.util.JavaMLReadable\n",
      " |      pyspark.ml.util.MLReadable\n",
      " |      typing.Generic\n",
      " |      pyspark.ml.util.JavaMLWritable\n",
      " |      pyspark.ml.util.MLWritable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, inputCols: Optional[List[str]] = None, outputCol: Optional[str] = None, handleInvalid: str = 'error')\n",
      " |      __init__(self, \\*, inputCols=None, outputCol=None, handleInvalid=\"error\")\n",
      " |  \n",
      " |  setHandleInvalid(self, value: str) -> 'VectorAssembler'\n",
      " |      Sets the value of :py:attr:`handleInvalid`.\n",
      " |  \n",
      " |  setInputCols(self, value: List[str]) -> 'VectorAssembler'\n",
      " |      Sets the value of :py:attr:`inputCols`.\n",
      " |  \n",
      " |  setOutputCol(self, value: str) -> 'VectorAssembler'\n",
      " |      Sets the value of :py:attr:`outputCol`.\n",
      " |  \n",
      " |  setParams(self, *, inputCols: Optional[List[str]] = None, outputCol: Optional[str] = None, handleInvalid: str = 'error') -> 'VectorAssembler'\n",
      " |      setParams(self, \\*, inputCols=None, outputCol=None, handleInvalid=\"error\")\n",
      " |      Sets params for this VectorAssembler.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'_input_kwargs': typing.Dict[str, typing.Any], 'han...\n",
      " |  \n",
      " |  __orig_bases__ = (<class 'pyspark.ml.wrapper.JavaTransformer'>, <class...\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  handleInvalid = Param(parent='undefined', name='handleInvalid', ...o d...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n",
      " |  \n",
      " |  clear(self, param: pyspark.ml.param.Param) -> None\n",
      " |      Clears a param from the param map if it has been explicitly set.\n",
      " |  \n",
      " |  copy(self: 'JP', extra: Optional[ForwardRef('ParamMap')] = None) -> 'JP'\n",
      " |      Creates a copy of this instance with the same uid and some\n",
      " |      extra params. This implementation first calls Params.copy and\n",
      " |      then make a copy of the companion Java pipeline component with\n",
      " |      extra params. So both the Python wrapper and the Java pipeline\n",
      " |      component get copied.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      extra : dict, optional\n",
      " |          Extra parameters to copy to the new instance\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :py:class:`JavaParams`\n",
      " |          Copy of this instance\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __del__(self) -> None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.base.Transformer:\n",
      " |  \n",
      " |  transform(self, dataset: pyspark.sql.dataframe.DataFrame, params: Optional[ForwardRef('ParamMap')] = None) -> pyspark.sql.dataframe.DataFrame\n",
      " |      Transforms the input dataset with optional parameters.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dataset : :py:class:`pyspark.sql.DataFrame`\n",
      " |          input dataset\n",
      " |      params : dict, optional\n",
      " |          an optional param map that overrides embedded params.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :py:class:`pyspark.sql.DataFrame`\n",
      " |          transformed dataset\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasInputCols:\n",
      " |  \n",
      " |  getInputCols(self) -> List[str]\n",
      " |      Gets the value of inputCols or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasInputCols:\n",
      " |  \n",
      " |  inputCols = Param(parent='undefined', name='inputCols', doc='input col...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasOutputCol:\n",
      " |  \n",
      " |  getOutputCol(self) -> str\n",
      " |      Gets the value of outputCol or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasOutputCol:\n",
      " |  \n",
      " |  outputCol = Param(parent='undefined', name='outputCol', doc='output co...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasHandleInvalid:\n",
      " |  \n",
      " |  getHandleInvalid(self) -> str\n",
      " |      Gets the value of handleInvalid or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  explainParam(self, param: Union[str, pyspark.ml.param.Param]) -> str\n",
      " |      Explains a single param and returns its name, doc, and optional\n",
      " |      default value and user-supplied value in a string.\n",
      " |  \n",
      " |  explainParams(self) -> str\n",
      " |      Returns the documentation of all params with their optionally\n",
      " |      default values and user-supplied values.\n",
      " |  \n",
      " |  extractParamMap(self, extra: Optional[ForwardRef('ParamMap')] = None) -> 'ParamMap'\n",
      " |      Extracts the embedded default param values and user-supplied\n",
      " |      values, and then merges them with extra values from input into\n",
      " |      a flat param map, where the latter value is used if there exist\n",
      " |      conflicts, i.e., with ordering: default param values <\n",
      " |      user-supplied values < extra.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      extra : dict, optional\n",
      " |          extra param values\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict\n",
      " |          merged param map\n",
      " |  \n",
      " |  getOrDefault(self, param: Union[str, pyspark.ml.param.Param[~T]]) -> Union[Any, ~T]\n",
      " |      Gets the value of a param in the user-supplied param map or its\n",
      " |      default value. Raises an error if neither is set.\n",
      " |  \n",
      " |  getParam(self, paramName: str) -> pyspark.ml.param.Param\n",
      " |      Gets a param by its name.\n",
      " |  \n",
      " |  hasDefault(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n",
      " |      Checks whether a param has a default value.\n",
      " |  \n",
      " |  hasParam(self, paramName: str) -> bool\n",
      " |      Tests whether this instance contains a param with a given\n",
      " |      (string) name.\n",
      " |  \n",
      " |  isDefined(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n",
      " |      Checks whether a param is explicitly set by user or has\n",
      " |      a default value.\n",
      " |  \n",
      " |  isSet(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n",
      " |      Checks whether a param is explicitly set by user.\n",
      " |  \n",
      " |  set(self, param: pyspark.ml.param.Param, value: Any) -> None\n",
      " |      Sets a parameter in the embedded param map.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  params\n",
      " |      Returns all params ordered by name. The default implementation\n",
      " |      uses :py:func:`dir` to get all attributes of type\n",
      " |      :py:class:`Param`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.Identifiable:\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n",
      " |  \n",
      " |  read() -> pyspark.ml.util.JavaMLReader[~RL] from abc.ABCMeta\n",
      " |      Returns an MLReader instance for this class.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.MLReadable:\n",
      " |  \n",
      " |  load(path: str) -> ~RL from abc.ABCMeta\n",
      " |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from abc.ABCMeta\n",
      " |      Parameterizes a generic class.\n",
      " |      \n",
      " |      At least, parameterizing a generic class is the *main* thing this method\n",
      " |      does. For example, for some generic class `Foo`, this is called when we\n",
      " |      do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n",
      " |      \n",
      " |      However, note that this method is also called when defining generic\n",
      " |      classes in the first place with `class Foo(Generic[T]): ...`.\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from abc.ABCMeta\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n",
      " |  \n",
      " |  write(self) -> pyspark.ml.util.JavaMLWriter\n",
      " |      Returns an MLWriter instance for this ML instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.MLWritable:\n",
      " |  \n",
      " |  save(self, path: str) -> None\n",
      " |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(VectorAssembler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols = data.columns,\n",
    "                            outputCol = \"features\",\n",
    "                            handleInvalid='skip')\n",
    "\n",
    "data = assembler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------+\n",
      "|features                                                                                          |\n",
      "+--------------------------------------------------------------------------------------------------+\n",
      "|(22,[2,3,4,5,9,10,12,14,15,16,19,20,21],[1.0,1.0,15.0,1.0,1.0,1.0,1.0,5.0,10.0,20.0,11.0,4.0,5.0])|\n",
      "|(22,[0,1,3,4,7,9,12,14,19,20,21],[2.0,1.0,1.0,28.0,1.0,1.0,1.0,2.0,11.0,4.0,3.0])                 |\n",
      "|[2.0,1.0,1.0,1.0,33.0,0.0,0.0,0.0,1.0,1.0,1.0,0.0,1.0,0.0,2.0,10.0,0.0,0.0,0.0,9.0,4.0,7.0]       |\n",
      "|[2.0,0.0,1.0,1.0,29.0,0.0,1.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,5.0,0.0,30.0,1.0,1.0,12.0,3.0,4.0]      |\n",
      "|(22,[3,4,5,12,14,17,18,19,20,21],[1.0,24.0,1.0,1.0,3.0,1.0,1.0,13.0,5.0,6.0])                     |\n",
      "+--------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('features').show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputCol: input column name. (undefined)\n",
      "outputCol: output column name. (default: StandardScaler_8f368e746c37__output)\n",
      "withMean: Center data with mean (default: False)\n",
      "withStd: Scale to unit standard deviation (default: True)\n"
     ]
    }
   ],
   "source": [
    "print(scaler.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol = \"features\",\n",
    "                        outputCol = \"scaled_features\",\n",
    "                        withStd = True,\n",
    "                        withMean = False)\n",
    "\n",
    "data = scaler.fit(data).transform(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|scaled_features                                                                                                                                                                                                                                                                                                                               |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(22,[2,3,4,5,9,10,12,14,15,16,19,20,21],[2.039505812822488,5.321720326327212,2.2893579599391347,2.031712728924379,2.0615392694368477,2.648422280086196,5.268510303114362,4.858456134508728,1.2679886362859858,2.425445791563413,3.398211452286401,4.227501950354121,2.104864706844905])                                                       |\n",
      "|(22,[0,1,3,4,7,9,12,14,19,20,21],[2.836729809501961,2.027066509160867,5.321720326327212,4.273468191886385,3.5565433958645047,2.0615392694368477,5.268510303114362,1.9433824538034912,3.398211452286401,4.227501950354121,1.262918824106943])                                                                                                  |\n",
      "|[2.836729809501961,2.027066509160867,2.039505812822488,5.321720326327212,5.036587511866097,0.0,0.0,0.0,2.410997335933827,2.0615392694368477,2.648422280086196,0.0,5.268510303114362,0.0,1.9433824538034912,1.2679886362859858,0.0,0.0,0.0,2.780354824597964,4.227501950354121,2.9468105895828667]                                             |\n",
      "|[2.836729809501961,0.0,2.039505812822488,5.321720326327212,4.426092055882328,0.0,5.171807519043947,3.5565433958645047,2.410997335933827,2.0615392694368477,2.648422280086196,0.0,5.268510303114362,0.0,4.858456134508728,0.0,3.6381686873451193,2.7708507836399017,2.0019658334254484,3.707139766130619,3.1706264627655907,1.6838917654759238]|\n",
      "|(22,[3,4,5,12,14,17,18,19,20,21],[5.321720326327212,3.662972735902616,2.031712728924379,5.268510303114362,2.9150736807052366,2.7708507836399017,2.0019658334254484,4.016068079974837,5.284377437942651,2.525837648213886])                                                                                                                    |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(col('scaled_features')).show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236378"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero vamos a probar con Kmeans a ver que tal nos salen los clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo  = KMeans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distanceMeasure: the distance measure. Supported options: 'euclidean' and 'cosine'. (default: euclidean)\n",
      "featuresCol: features column name. (default: features)\n",
      "initMode: The initialization algorithm. This can be either \"random\" to choose random points as initial cluster centers, or \"k-means||\" to use a parallel variant of k-means++ (default: k-means||)\n",
      "initSteps: The number of steps for k-means|| initialization mode. Must be > 0. (default: 2)\n",
      "k: The number of clusters to create. Must be > 1. (default: 2)\n",
      "maxBlockSizeInMB: maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0. (default: 0.0)\n",
      "maxIter: max number of iterations (>= 0). (default: 20)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "seed: random seed. (default: 4645095874236920005)\n",
      "solver: The solver algorithm for optimization. Supported options: auto, row, block. (default: auto)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 0.0001)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "print(modelo.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(distanceMeasure='euclidean',\n",
    "                featuresCol='scaled_features',\n",
    "                initMode= \"k-means||\",\n",
    "                initSteps= 1,\n",
    "                k= 3,\n",
    "                maxIter=50,\n",
    "                predictionCol=\"prediccion\",\n",
    "                seed= 12345678910,\n",
    "                solver='block',\n",
    "                tol= 0.0001,\n",
    "                weightCol='Diabetes_012')\n",
    "\n",
    "model = kmeans.fit(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = model.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+\n",
      "|Diabetes_012|prediccion|\n",
      "+------------+----------+\n",
      "|         0.0|         2|\n",
      "|         2.0|         1|\n",
      "|         2.0|         0|\n",
      "|         2.0|         2|\n",
      "|         0.0|         1|\n",
      "|         0.0|         2|\n",
      "|         0.0|         1|\n",
      "|         2.0|         0|\n",
      "|         0.0|         1|\n",
      "|         0.0|         1|\n",
      "|         2.0|         0|\n",
      "|         2.0|         1|\n",
      "|         2.0|         2|\n",
      "|         2.0|         1|\n",
      "|         0.0|         1|\n",
      "|         0.0|         1|\n",
      "|         0.0|         0|\n",
      "|         0.0|         0|\n",
      "|         1.0|         1|\n",
      "|         0.0|         0|\n",
      "+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(cluster.select(col('Diabetes_012'),col('prediccion')).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.28243737439973304\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Calcula la métrica de silueta\n",
    "evaluator = ClusteringEvaluator(predictionCol='prediccion')\n",
    "silhouette = evaluator.evaluate(cluster)\n",
    "\n",
    "# Muestra el valor de la métrica de silueta\n",
    "print(\"Silhouette Score:\", silhouette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(distanceMeasure='euclidean',\n",
    "                featuresCol='scaled_features',\n",
    "                initMode= \"k-means||\",\n",
    "                initSteps= 1,\n",
    "                k= 3,\n",
    "                maxIter=50,\n",
    "                predictionCol=\"prediccion\",\n",
    "                seed= 12345678910,\n",
    "                solver='block',\n",
    "                tol= 0.0001,\n",
    "                weightCol='Diabetes_012')\n",
    "\n",
    "model = kmeans.fit(data)\n",
    "cluster = model.transform(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_kmeans(distance, init, initS, maxIt, solv, data):\n",
    "    kmeans = KMeans(distanceMeasure=distance,\n",
    "                    featuresCol='scaled_features',\n",
    "                    initMode=init,\n",
    "                    initSteps=initS,\n",
    "                    k=3,\n",
    "                    maxIter=maxIt,\n",
    "                    predictionCol=\"prediccion\",\n",
    "                    seed=12345678910,\n",
    "                    solver=solv,\n",
    "                    tol=0.0001,\n",
    "                    weightCol='Diabetes_012')\n",
    "\n",
    "    model = kmeans.fit(data)\n",
    "    \n",
    "    # Calcula la métrica de silueta\n",
    "    evaluator = ClusteringEvaluator(predictionCol='prediccion')\n",
    "    silhouette = evaluator.evaluate(model.transform(data))\n",
    "    \n",
    "    return silhouette\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distanceMeasure: the distance measure. Supported options: 'euclidean' and 'cosine'. (default: euclidean)\n",
      "featuresCol: features column name. (default: features)\n",
      "initMode: The initialization algorithm. This can be either \"random\" to choose random points as initial cluster centers, or \"k-means||\" to use a parallel variant of k-means++ (default: k-means||)\n",
      "initSteps: The number of steps for k-means|| initialization mode. Must be > 0. (default: 2)\n",
      "k: The number of clusters to create. Must be > 1. (default: 2)\n",
      "maxBlockSizeInMB: maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0. (default: 0.0)\n",
      "maxIter: max number of iterations (>= 0). (default: 20)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "seed: random seed. (default: 4645095874236920005)\n",
      "solver: The solver algorithm for optimization. Supported options: auto, row, block. (default: auto)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 0.0001)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "print(modelo.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def c_kmeans(distanceMeasure,initMode,initSteps,maxIter,solver):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30299709537434366\n"
     ]
    }
   ],
   "source": [
    "print(c_kmeans('cosine','k-means||',2,1,'row',data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30299709537434366\n"
     ]
    }
   ],
   "source": [
    "print(c_kmeans('cosine','k-means||',2,1,'block',data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.325973305587922\n"
     ]
    }
   ],
   "source": [
    "print(c_kmeans('euclidean','k-means||',2,1,'block',data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.325973305587922\n"
     ]
    }
   ],
   "source": [
    "print(c_kmeans('euclidean','k-means||',2,1,'block',data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bisecting K-Means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = spark.read.csv(path= 'Data/Diabetes.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import BisectingKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = BisectingKMeans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distanceMeasure: the distance measure. Supported options: 'euclidean' and 'cosine'. (default: euclidean)\n",
      "featuresCol: features column name. (default: features)\n",
      "k: The desired number of leaf clusters. Must be > 1. (default: 4)\n",
      "maxIter: max number of iterations (>= 0). (default: 20)\n",
      "minDivisibleClusterSize: The minimum number of points (if >= 1.0) or the minimum proportion of points (if < 1.0) of a divisible cluster. (default: 1.0)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "seed: random seed. (default: -3433244116123162505)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "print(mod.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols = data2.columns,\n",
    "                            outputCol = \"features\",\n",
    "                            handleInvalid='skip')\n",
    "\n",
    "data2 = assembler.transform(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol = \"features\",\n",
    "                        outputCol = \"scaled_features\",\n",
    "                        withStd = True,\n",
    "                        withMean = False)\n",
    "\n",
    "data2 = scaler.fit(data2).transform(data2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "bkmeans = BisectingKMeans(featuresCol    = \"scaled_features\",\n",
    "                         predictionCol   = \"bkcluster\", \n",
    "                         k               = 3,\n",
    "                         distanceMeasure = \"euclidean\",\n",
    "                         seed = 12345678910\n",
    "                         \n",
    "                         )\n",
    "\n",
    "model = bkmeans.fit(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inertia: -0.015275344248975885\n"
     ]
    }
   ],
   "source": [
    "# Realizar predicciones en los datos\n",
    "predictions = model.transform(data2)\n",
    "\n",
    "# Calcular la inercia\n",
    "evaluator = ClusteringEvaluator(predictionCol='bkcluster')\n",
    "inertia = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Inertia:\", inertia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con bkmeans no me salen bien la inercia de los clusters voy a probar hacer el modelo de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(path= 'Data/Diabetes.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = data.randomSplit(weights = [0.7, 0.3], seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "train, test = data.randomSplit(weights = [0.7, 0.3], seed = 42)\n",
    "# Ensamblar las características en un vector denso\n",
    "assembler = VectorAssembler(inputCols=data.columns, outputCol=\"raw_features\")\n",
    "\n",
    "# Normalizar las características\n",
    "scaler = StandardScaler(inputCol=\"raw_features\", outputCol=\"features\", withStd=True, withMean=False)\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y prueba\n",
    "train, test = data.randomSplit(weights=[0.7, 0.3], seed=42)\n",
    "\n",
    "# Definir el clasificador de árbol de decisión\n",
    "dtc = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"Diabetes_012\", predictionCol=\"prediction\", maxDepth=2)\n",
    "\n",
    "# Crear el pipeline\n",
    "pipeline = Pipeline(stages=[assembler, scaler, dtc])\n",
    "\n",
    "# Entrenar el modelo\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "# Hacer predicciones en el conjunto de prueba\n",
    "predictions = model.transform(test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+\n",
      "|Diabetes_012|prediction|\n",
      "+------------+----------+\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "|         1.0|       1.0|\n",
      "+------------+----------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.filter(col('Diabetes_012')==1.0).select(col('Diabetes_012'),col('prediction')).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Crear un evaluador de clasificación multiclase\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Diabetes_012\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Calcular la precisión en el conjunto de predicciones\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "# Imprimir la precisión\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol = \"Diabetes_012\",\n",
    "                                              predictionCol = \"prediction\",\n",
    "                                              metricName = \"f1\")\n",
    "\n",
    "f1 = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assembler = VectorAssembler(inputCols=data.columns, outputCol=\"features\")\n",
    "data = assembler.transform(data)\n",
    "train, test = data.randomSplit(weights=[0.7, 0.3], seed=42)\n",
    "\n",
    "# Definir el clasificador de árbol de decisión\n",
    "dtc = DecisionTreeClassifier(featuresCol='features', labelCol=\"Diabetes_012\", predictionCol=\"prediction\", maxDepth=2)\n",
    "\n",
    "model = dtc.fit(train)\n",
    "\n",
    "y_hat = model.transform(test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Crear un evaluador de clasificación multiclase\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Diabetes_012\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Calcular la precisión en el conjunto de predicciones\n",
    "accuracy = evaluator.evaluate(y_hat)\n",
    "\n",
    "# Imprimir la precisión\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
